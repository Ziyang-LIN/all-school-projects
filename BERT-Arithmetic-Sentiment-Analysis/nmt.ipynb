{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd",
        "colab_type": "text"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignmentby selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5",
        "colab_type": "text"
      },
      "source": [
        "## Setup PyTorch\n",
        "All files are stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbuunY8UdTB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow==4.0.0\n",
        "%mkdir -p /content/csc421/a3/\n",
        "%cd /content/csc421/a3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7",
        "colab_type": "text"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg",
        "colab_type": "text"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-UJHBYZkh7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(fname,\n",
        "             origin,\n",
        "             untar=False,\n",
        "             extract=False,\n",
        "             archive_format='auto',\n",
        "             cache_dir='data'):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + '.tar.gz'\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "    \n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print('Downloading data from', origin)\n",
        "\n",
        "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print('Extracting file.')\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "        \n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1",
        "colab_type": "text"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVT4TNTOV3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\n",
        "    \"\"\"\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa5-onJhoSeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      ## slow decoding, recompute everything at each time\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] #latest output token\n",
        "      \n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "    \n",
        "    if isinstance(attention_weights, tuple):\n",
        "      ## transformer's attention mweights\n",
        "      attention_weights, self_attention_weights = attention_weights\n",
        "    \n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "    \n",
        "    for i in range(len(all_attention_weights)):\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "      fig = plt.figure()\n",
        "      ax = fig.add_subplot(111)\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
        "      fig.colorbar(cax)\n",
        "\n",
        "      # Set up axes\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
        "\n",
        "      # Show label at every tick\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      # Add title\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
        "      plt.tight_layout()\n",
        "      plt.grid('off')\n",
        "      plt.show()\n",
        "      #plt.savefig(save)\n",
        "\n",
        "      #plt.close(fig)\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "              # Zero gradients\n",
        "              optimizer.zero_grad()\n",
        "              # Compute gradients\n",
        "              loss.backward()\n",
        "              # Update the parameters of the encoder and decoder\n",
        "              optimizer.step()\n",
        "              \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "  \n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "      encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                          hidden_size=opts.hidden_size, \n",
        "                          opts=opts)\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "      encoder = TransformerEncoder(vocab_size=vocab_size, \n",
        "                                   hidden_size=opts.hidden_size, \n",
        "                                   num_layers=opts.num_transformer_layers,\n",
        "                                   opts=opts)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size, \n",
        "                                      attention_type=opts.attention_type)\n",
        "    elif opts.decoder_type == 'transformer':\n",
        "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
        "                                     hidden_size=opts.hidden_size, \n",
        "                                     num_layers=opts.num_transformer_layers)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30",
        "colab_type": "text"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dec18adf-4671-4ce3-e0d1-f18d9650d1e0"
      },
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(fname='pig_latin_data.txt', \n",
        "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n",
        "                         untar=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/pig_latin_data.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: Gated Recurrent Unit (GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the Gated Recurent Unit class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HMO7FD6l5RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        ## Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Win = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        ## Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = F.tanh(self.Win(x) + r * self.Whn(h_prev))\n",
        "        h_new = (1 - z) * g + z * h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDNim2fmVJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvwizYM9ma4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None        \n",
        "        \"\"\"        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model comprised of recurrent encoder and decoders. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30b57eb3-3f90-4d4f-f7ad-ede54b530253"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': '',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stage', 'agestay')\n",
            "('breach', 'eachbray')\n",
            "('thirty-six', 'irtythay-ixsay')\n",
            "('lodges', 'odgeslay')\n",
            "('itself', 'itselfway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.283 | Val loss: 1.994 | Gen: ay ay ontay onsay ontay\n",
            "Epoch:   1 | Train loss: 1.860 | Val loss: 1.869 | Gen: ansay ay onsay ansay ontay\n",
            "Epoch:   2 | Train loss: 1.723 | Val loss: 1.795 | Gen: essay aray onsingway insay-ay oontingway\n",
            "Epoch:   3 | Train loss: 1.630 | Val loss: 1.741 | Gen: essay away ontay-atingay-ayday issay ontay-ayday\n",
            "Epoch:   4 | Train loss: 1.547 | Val loss: 1.692 | Gen: essay away-away onsingway-ayday issay-ayday oontay-away-ayday\n",
            "Epoch:   5 | Train loss: 1.483 | Val loss: 1.634 | Gen: essay away-ayday onsingedway isssay oontay-away\n",
            "Epoch:   6 | Train loss: 1.423 | Val loss: 1.608 | Gen: eway away-ayday ontionsay-athay-ayda issay-ayday otingway\n",
            "Epoch:   7 | Train loss: 1.377 | Val loss: 1.586 | Gen: eway away-ayday ondentionday isay-ayday oodgay-athedway\n",
            "Epoch:   8 | Train loss: 1.333 | Val loss: 1.576 | Gen: eway ay-ay-ayday ondentionday isay oonway-ayday\n",
            "Epoch:   9 | Train loss: 1.303 | Val loss: 1.526 | Gen: eway ay-ay-ayday ongay-ingway isay-ayday othay-ayday\n",
            "Epoch:  10 | Train loss: 1.273 | Val loss: 1.566 | Gen: eway aingway onitiondenthingay-at isay-ayday ongay-ingway-ayday\n",
            "Epoch:  11 | Train loss: 1.242 | Val loss: 1.510 | Gen: eway aingway ontitionday-ayday ishay oway-away-ayday\n",
            "Epoch:  12 | Train loss: 1.204 | Val loss: 1.480 | Gen: eway ay-ay-away-ayday ongentionday-ayday ishay oway-ayday-ayday\n",
            "Epoch:  13 | Train loss: 1.171 | Val loss: 1.478 | Gen: eway aingway ontitionsedway ishay oway-ansingsay\n",
            "Epoch:  14 | Train loss: 1.155 | Val loss: 1.492 | Gen: eway aiway ondingentionsay issay ooughay-anway-ancewa\n",
            "Epoch:  15 | Train loss: 1.140 | Val loss: 1.440 | Gen: eway ayway-ansingsay onderitioncay issway oway-away-aysway\n",
            "Epoch:  16 | Train loss: 1.112 | Val loss: 1.399 | Gen: eway aiway ontitionsedway ishay oway-away-ayday\n",
            "Epoch:  17 | Train loss: 1.100 | Val loss: 1.493 | Gen: eway aiway ontitiontiontentway- issway owingway-ayday\n",
            "Epoch:  18 | Train loss: 1.083 | Val loss: 1.366 | Gen: eway aiway ongingsay-indentway issway okgay-indway-ayday\n",
            "Epoch:  19 | Train loss: 1.067 | Val loss: 1.412 | Gen: eway ay-ayday-ayday ongitionsay-ayday issway oidshay\n",
            "Epoch:  20 | Train loss: 1.054 | Val loss: 1.348 | Gen: eway aiway onitioncedway issway okgay-ashay-ayssay\n",
            "Epoch:  21 | Train loss: 1.032 | Val loss: 1.422 | Gen: eway aiway onitientionlay-ayday ishay okgay-anway-aydsay\n",
            "Epoch:  22 | Train loss: 1.020 | Val loss: 1.331 | Gen: eway aiway ongeritioncay isssay ookgay-anway-aydsay\n",
            "Epoch:  23 | Train loss: 1.010 | Val loss: 1.385 | Gen: eway aiway ongitionday-aysday issway oxhay-away-ayday\n",
            "Epoch:  24 | Train loss: 1.008 | Val loss: 1.381 | Gen: eway aiway ongeritioncay issway oikgay-ayday\n",
            "Epoch:  25 | Train loss: 0.986 | Val loss: 1.322 | Gen: eway aiway ongitionsay-ayday issway okingay\n",
            "Epoch:  26 | Train loss: 0.978 | Val loss: 1.462 | Gen: eway aiway ongeritionday issway oikway-ayday\n",
            "Epoch:  27 | Train loss: 0.989 | Val loss: 1.300 | Gen: eway aiway ongingentionday ishay oxgay-away-ayday\n",
            "Epoch:  28 | Train loss: 0.958 | Val loss: 1.299 | Gen: eway aiway onitiondentway-ayday ishay oikgay-ayday\n",
            "Epoch:  29 | Train loss: 0.959 | Val loss: 1.576 | Gen: eway aiway ongeritionitiondentw ishay oxgay-anway-away-awl\n",
            "Epoch:  30 | Train loss: 0.965 | Val loss: 1.292 | Gen: eway aiway ongitionceway ishay oidshay-ayday\n",
            "Epoch:  31 | Train loss: 0.937 | Val loss: 1.278 | Gen: eway aiway ongitiondlay-awlay issway oxgay-anway-away-awl\n",
            "Epoch:  32 | Train loss: 0.932 | Val loss: 1.400 | Gen: eway airway onguriencay isway oikgray-aysday\n",
            "Epoch:  33 | Train loss: 0.935 | Val loss: 1.400 | Gen: eway aiway ongtingray-ownay ishay oway-ay-ay-ay-ay-ayd\n",
            "Epoch:  34 | Train loss: 0.941 | Val loss: 1.236 | Gen: eway aiway ongitingentway-yeara issway oxgay-anway-away-awl\n",
            "Epoch:  35 | Train loss: 0.897 | Val loss: 1.218 | Gen: eway aiway onitientionday issway oxgay-answay-ayday\n",
            "Epoch:  36 | Train loss: 0.895 | Val loss: 1.291 | Gen: eway aiway ongitionstingay ishay oxgay-anscay\n",
            "Epoch:  37 | Train loss: 0.890 | Val loss: 1.422 | Gen: eway aiway onitiedingedday-ayda isway oidway-anway-ayday\n",
            "Epoch:  38 | Train loss: 0.899 | Val loss: 1.271 | Gen: eway aiway ongurerationcay isway oxgay-ansshay\n",
            "Epoch:  39 | Train loss: 0.886 | Val loss: 1.352 | Gen: eway aiwray ongitioncay isway oxgay-ashay\n",
            "Epoch:  40 | Train loss: 0.896 | Val loss: 1.387 | Gen: ehway-ayday aiway ongititiondlay issway owingsay\n",
            "Epoch:  41 | Train loss: 0.882 | Val loss: 1.233 | Gen: eway aiway onitiondionday isway oxgay-away-aysday\n",
            "Epoch:  42 | Train loss: 0.868 | Val loss: 1.313 | Gen: eway aiwray ongitingedway-ybay ishay oxgay-andway-ybay\n",
            "Epoch:  43 | Train loss: 0.860 | Val loss: 1.250 | Gen: eway aiwray ongutionioushay-ayda isway oxgaimay\n",
            "Epoch:  44 | Train loss: 0.836 | Val loss: 1.264 | Gen: eway aiwray ongutiniontay-estay- issway owingsay\n",
            "Epoch:  45 | Train loss: 0.848 | Val loss: 1.275 | Gen: eway aiway ongitiontway issway oxgay-ansingway\n",
            "Epoch:  46 | Train loss: 0.873 | Val loss: 1.434 | Gen: ehway ariwmay onguitionsay-awlay ishay orhaidsway\n",
            "Epoch:  47 | Train loss: 0.890 | Val loss: 1.243 | Gen: eway aiway ongitiondlay issway oxgainway\n",
            "Epoch:  48 | Train loss: 0.856 | Val loss: 1.215 | Gen: eway aiwray onitingentsay issway oidshay\n",
            "Epoch:  49 | Train loss: 0.826 | Val loss: 1.246 | Gen: eway airway onditinionsay-oway-a isway oxgay-ashsay\n",
            "Epoch:  50 | Train loss: 0.813 | Val loss: 1.243 | Gen: eway airway ongutingedtay-ortay- ishay oidway-anwimay\n",
            "Epoch:  51 | Train loss: 0.808 | Val loss: 1.202 | Gen: ehay airway onditingray-esentway isway oxgaisway\n",
            "Epoch:  52 | Train loss: 0.797 | Val loss: 1.224 | Gen: eway airway ongutionceway isway oidway-anshay\n",
            "Epoch:  53 | Train loss: 0.810 | Val loss: 1.297 | Gen: ehay-ayday aiway onditientionday isway owingay-ayday\n",
            "Epoch:  54 | Train loss: 0.820 | Val loss: 1.307 | Gen: ehay aiwray onditingsay-owlay-ay isway oxgaisway\n",
            "Epoch:  55 | Train loss: 0.823 | Val loss: 1.408 | Gen: ehay airway ongrationscray issway orrighay\n",
            "Epoch:  56 | Train loss: 0.825 | Val loss: 1.208 | Gen: eway aiway onditingencay isway oidway-anshay\n",
            "Epoch:  57 | Train loss: 0.798 | Val loss: 1.219 | Gen: ehay aiwray onditingray-easedway isway oxgaisway\n",
            "Epoch:  58 | Train loss: 0.785 | Val loss: 1.171 | Gen: eway airway onditingedcay isway oxgaincay\n",
            "Epoch:  59 | Train loss: 0.772 | Val loss: 1.202 | Gen: ehay airway onditingrableway isway oidway-anshay\n",
            "Epoch:  60 | Train loss: 0.775 | Val loss: 1.259 | Gen: eway airway onditiontionday isway oidway-andssay\n",
            "Epoch:  61 | Train loss: 0.783 | Val loss: 1.279 | Gen: ehay airway onditingray-oraceway ishay oxgaishay\n",
            "Epoch:  62 | Train loss: 0.784 | Val loss: 1.252 | Gen: ehay airway onditingray-easedway isway ohingsay\n",
            "Epoch:  63 | Train loss: 0.774 | Val loss: 1.187 | Gen: ehay airway onditingray-eateway isway oidway-anshay\n",
            "Epoch:  64 | Train loss: 0.765 | Val loss: 1.159 | Gen: ehay airway onditinitenestay ishay oidrinsway\n",
            "Epoch:  65 | Train loss: 0.757 | Val loss: 1.175 | Gen: eway-ybay airway onditingenceway isway oxgaickay\n",
            "Epoch:  66 | Train loss: 0.753 | Val loss: 1.315 | Gen: ehtway airway onditingrablelesway ishay oidway-anshgay\n",
            "Epoch:  67 | Train loss: 0.754 | Val loss: 1.150 | Gen: ehay airway onditingedsay isway oxgaickay\n",
            "Epoch:  68 | Train loss: 0.747 | Val loss: 1.190 | Gen: ehtway airway onditinitenestway isway oidway-aysdchay\n",
            "Epoch:  69 | Train loss: 0.745 | Val loss: 1.318 | Gen: ehay-ayday airway onditinglelionway isway oidway-anshshay\n",
            "Epoch:  70 | Train loss: 0.754 | Val loss: 1.204 | Gen: ehtway airway onditingray-esentway isway oidway-ansiwmay\n",
            "Epoch:  71 | Train loss: 0.739 | Val loss: 1.278 | Gen: ehhay airway onditinglationway isway oidway-andshay\n",
            "Epoch:  72 | Train loss: 0.734 | Val loss: 1.136 | Gen: ehay airway onditingnestway-ofwa isway oingway-aypay\n",
            "Epoch:  73 | Train loss: 0.739 | Val loss: 1.311 | Gen: ehay airway onditentionionway isway oidway-aysday\n",
            "Epoch:  74 | Train loss: 0.753 | Val loss: 1.270 | Gen: ehway airway onditinglay-estended isway oidway-aysdray\n",
            "Epoch:  75 | Train loss: 0.740 | Val loss: 1.166 | Gen: ehay airway onditingedcay isway oidway-answay\n",
            "Epoch:  76 | Train loss: 0.730 | Val loss: 1.243 | Gen: ehay airway ondingringspray ishay oidray-away-aypay\n",
            "Epoch:  77 | Train loss: 0.728 | Val loss: 1.236 | Gen: ehay airway onditingray-esentway isway oidway-insway\n",
            "Epoch:  78 | Train loss: 0.727 | Val loss: 1.230 | Gen: ehay airway onditionionsway ishay oidchway\n",
            "Epoch:  79 | Train loss: 0.725 | Val loss: 1.207 | Gen: ehay airway onditinionionway ishay oidway-aysday\n",
            "Epoch:  80 | Train loss: 0.720 | Val loss: 1.223 | Gen: ehay airway onditingray-esentway isway oindway-anshgay\n",
            "Epoch:  81 | Train loss: 0.709 | Val loss: 1.196 | Gen: ehay airway onditinglay-estentwa isway oindway-ay-tray\n",
            "Epoch:  82 | Train loss: 0.699 | Val loss: 1.141 | Gen: ehay airway onditingray-eableway isway oidway-aysdray\n",
            "Epoch:  83 | Train loss: 0.694 | Val loss: 1.125 | Gen: ehay airway onditinglay-otay-ese isway oindway-ay-tmay-inwa\n",
            "Epoch:  84 | Train loss: 0.686 | Val loss: 1.147 | Gen: ehay airway onditionionsway isway oindway-aysdray\n",
            "Epoch:  85 | Train loss: 0.688 | Val loss: 1.129 | Gen: ehay airway onditingplay isway oindway-ay-xmay\n",
            "Epoch:  86 | Train loss: 0.688 | Val loss: 1.189 | Gen: ehay airway onditingray-eatedpay isway oindway-anwmay\n",
            "Epoch:  87 | Train loss: 0.689 | Val loss: 1.186 | Gen: ehay airway onditingpray-away-ek isway oindway-anshmay\n",
            "Epoch:  88 | Train loss: 0.699 | Val loss: 1.262 | Gen: ehhay airway onditioniondway-ofwa isway oindway-anshay\n",
            "Epoch:  89 | Train loss: 0.703 | Val loss: 1.123 | Gen: ehay airway onditingedsay isway oindway-ay-dray\n",
            "Epoch:  90 | Train loss: 0.675 | Val loss: 1.145 | Gen: ehay airway onditingreday isway oidway-aysdcay\n",
            "Epoch:  91 | Train loss: 0.673 | Val loss: 1.204 | Gen: ehay airway onditinglay-eatesway isway oindway-ay-tmay-owna\n",
            "Epoch:  92 | Train loss: 0.674 | Val loss: 1.213 | Gen: ehay airway onditinglestway-ybay isway oidway-answay\n",
            "Epoch:  93 | Train loss: 0.686 | Val loss: 1.203 | Gen: ehay airway ondingriblestway isway orkingway\n",
            "Epoch:  94 | Train loss: 0.696 | Val loss: 1.247 | Gen: ehway airway onditingnestingway isway oikingway\n",
            "Epoch:  95 | Train loss: 0.680 | Val loss: 1.136 | Gen: ehway airway onditionionway issay ohingsay\n",
            "Epoch:  96 | Train loss: 0.682 | Val loss: 1.181 | Gen: ehay airway onditiontionway isway okingway\n",
            "Epoch:  97 | Train loss: 0.677 | Val loss: 1.211 | Gen: ehay airway onditingpray isway ohingsay\n",
            "Epoch:  98 | Train loss: 0.677 | Val loss: 1.290 | Gen: ehay airway onditingrucedway ishay ohingsay\n",
            "Epoch:  99 | Train loss: 0.667 | Val loss: 1.159 | Gen: ehay airway onditingventway isway ohingsay\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay airway onditingventway isway ohingsay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt",
        "colab_type": "text"
      },
      "source": [
        "Try translating different sentences by changing the variable TEST_SENTENCE. Identify two distinct failure modes and briefly describe them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e8959b4b-994a-4ceb-a942-518ac5af8a8a"
      },
      "source": [
        "TEST_SENTENCE = 'but the heat pump is not functioning'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "TEST_SENTENCE2 = 'shake thack ay way sss iii hyperparameters'\n",
        "translated = translate_sentence(TEST_SENTENCE2, rnn_encoder, rnn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE2, translated))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tbut the heat pump is not functioning \n",
            "translated:\tutway ehay eathay umppay isway otay untorrionsway\n",
            "source:\t\tshake thack ay way sss iii hyperparameters \n",
            "translated:\takesway acktray ayway ayway fsay iriway uscalicationcay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Additive Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Additive Attention\n",
        "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdewEVSMo5jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "                                    nn.Linear(hidden_size*2, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Linear(hidden_size, 1)\n",
        "                                 )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
        "        return context, attention_weights\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: RNN Additive Attention Decoder\n",
        "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJaABkXrpJSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        if attention_type == 'additive':\n",
        "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == 'scaled_dot':\n",
        "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            # ------------\n",
        "            # FILL THIS IN - START\n",
        "            # ------------\n",
        "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat((embed_current, torch.squeeze(context, 1)), dim=1)  # batch_size x (2*hidden_size) \n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size      \n",
        "            # ------------\n",
        "            # FILL THIS IN - END\n",
        "            # ------------     \n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3-FuzY1pepu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1214fc75-496e-41d1-82e6-ec065db27dc5"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005, \n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'rnn', # options: rnn / transformer\n",
        "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
        "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "rnn_attn_encoder, rnn_attn_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stage', 'agestay')\n",
            "('breach', 'eachbray')\n",
            "('thirty-six', 'irtythay-ixsay')\n",
            "('lodges', 'odgeslay')\n",
            "('itself', 'itselfway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.305 | Val loss: 2.037 | Gen: entay ay ongay-onay-ontay-ay- ay ongnay-onday-onday-o\n",
            "Epoch:   1 | Train loss: 1.837 | Val loss: 1.819 | Gen: elay-edway aray ongnay-ongay-ay-enta ay ongray-ongray-ongray\n",
            "Epoch:   2 | Train loss: 1.614 | Val loss: 1.666 | Gen: eday aray intingay-ingay-ay-en isay encongray-ongray-ong\n",
            "Epoch:   3 | Train loss: 1.450 | Val loss: 1.578 | Gen: edway arisay ontentay-inway istay orgray-ongray\n",
            "Epoch:   4 | Train loss: 1.327 | Val loss: 1.523 | Gen: edhtay aray ontentay-ay-ay-ay-ay ay ornay-oray-y-ay\n",
            "Epoch:   5 | Train loss: 1.216 | Val loss: 1.362 | Gen: edtay airay ongntingay-ingay-inw isay ornway-oray-ingray\n",
            "Epoch:   6 | Train loss: 1.101 | Val loss: 1.259 | Gen: edway arartway ondingay-inway isay orgnay-ongray\n",
            "Epoch:   7 | Train loss: 0.993 | Val loss: 1.252 | Gen: eway airay ondingay-inway isay orhongray\n",
            "Epoch:   8 | Train loss: 0.907 | Val loss: 1.264 | Gen: eththay airway ondintiontway issay ordningray\n",
            "Epoch:   9 | Train loss: 0.851 | Val loss: 1.274 | Gen: iway airway ondionway isay ordingway\n",
            "Epoch:  10 | Train loss: 0.788 | Val loss: 1.122 | Gen: ewhay airway ondiway-inionway isay ordingway-ingway\n",
            "Epoch:  11 | Train loss: 0.743 | Val loss: 1.041 | Gen: eway airway ondingeninenway isway ordingway\n",
            "Epoch:  12 | Train loss: 0.684 | Val loss: 1.529 | Gen: elthay airraway ondindiociongway issay omromungnway\n",
            "Epoch:  13 | Train loss: 0.661 | Val loss: 0.841 | Gen: eway airay onditioinginingay-in issay orwakingway\n",
            "Epoch:  14 | Train loss: 0.489 | Val loss: 0.936 | Gen: eway airway ondiotingway isway orknnway\n",
            "Epoch:  15 | Train loss: 0.424 | Val loss: 0.667 | Gen: eway abray onditioneninginencay isway orwakenway\n",
            "Epoch:  16 | Train loss: 0.403 | Val loss: 0.616 | Gen: eway airway onditionininway-inin isway orkingway\n",
            "Epoch:  17 | Train loss: 0.329 | Val loss: 0.607 | Gen: eway airway onditioningingay-ine isway orwakingway\n",
            "Epoch:  18 | Train loss: 0.349 | Val loss: 1.227 | Gen: ethay arirway ondicengingway isway orway-ingway\n",
            "Epoch:  19 | Train loss: 0.418 | Val loss: 1.014 | Gen: eway airway ondcay-ingway isway ongnay\n",
            "Epoch:  20 | Train loss: 0.404 | Val loss: 0.478 | Gen: eway airway onditiongay-ingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.229 | Val loss: 0.436 | Gen: eway airway onditiongcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.193 | Val loss: 0.366 | Gen: ethay airway ondiotiningingcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.159 | Val loss: 0.457 | Gen: elay airway ondiotiongingcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.150 | Val loss: 0.832 | Gen: ethay airway onditioninignway isway orkingway\n",
            "Epoch:  25 | Train loss: 0.162 | Val loss: 0.441 | Gen: ethay airway onditiongingcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.168 | Val loss: 0.949 | Gen: etthay airway ondiongncay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.212 | Val loss: 0.636 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  28 | Train loss: 0.219 | Val loss: 0.428 | Gen: etay airway onditioningningway isway orkingway\n",
            "Epoch:  29 | Train loss: 0.129 | Val loss: 0.345 | Gen: ethay airay onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.100 | Val loss: 0.241 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.078 | Val loss: 0.224 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.061 | Val loss: 0.188 | Gen: etay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.055 | Val loss: 0.269 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.067 | Val loss: 0.247 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  35 | Train loss: 0.112 | Val loss: 0.517 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  36 | Train loss: 0.200 | Val loss: 0.715 | Gen: etay airway onditionnnnnnway isway orkingway\n",
            "Epoch:  37 | Train loss: 0.201 | Val loss: 0.393 | Gen: ethay airway onditioningingnay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.103 | Val loss: 0.255 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.073 | Val loss: 0.247 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.049 | Val loss: 0.163 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.036 | Val loss: 0.140 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.029 | Val loss: 0.130 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.025 | Val loss: 0.145 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.024 | Val loss: 0.125 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  45 | Train loss: 0.022 | Val loss: 0.130 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  46 | Train loss: 0.023 | Val loss: 0.112 | Gen: etay airway onditioningway isway orkingway\n",
            "Epoch:  47 | Train loss: 0.020 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.071 | Val loss: 0.775 | Gen: ethay airway onditioningcay iway orkingway\n",
            "Epoch:  49 | Train loss: 0.144 | Val loss: 0.298 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.068 | Val loss: 0.356 | Gen: elay airway onditioningway isway orkingway\n",
            "Epoch:  51 | Train loss: 0.156 | Val loss: 0.628 | Gen: tay airway onditioningingway isway orkingway\n",
            "Epoch:  52 | Train loss: 0.187 | Val loss: 0.770 | Gen: ethay airway ontitiongway isway orkingway\n",
            "Epoch:  53 | Train loss: 0.103 | Val loss: 0.402 | Gen: ethay airway onditioniningcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.052 | Val loss: 0.138 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.028 | Val loss: 0.172 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.023 | Val loss: 0.105 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.017 | Val loss: 0.111 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.014 | Val loss: 0.127 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.013 | Val loss: 0.122 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  60 | Train loss: 0.011 | Val loss: 0.109 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.010 | Val loss: 0.104 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.009 | Val loss: 0.099 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.008 | Val loss: 0.098 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.007 | Val loss: 0.095 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.007 | Val loss: 0.096 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.006 | Val loss: 0.095 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.006 | Val loss: 0.094 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.005 | Val loss: 0.094 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.005 | Val loss: 0.096 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.005 | Val loss: 0.101 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.015 | Val loss: 0.152 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.123 | Val loss: 0.788 | Gen: ethay airway ondionvnnay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.143 | Val loss: 0.405 | Gen: eway airway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.059 | Val loss: 0.116 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  75 | Train loss: 0.019 | Val loss: 0.115 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.013 | Val loss: 0.105 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  77 | Train loss: 0.010 | Val loss: 0.090 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.007 | Val loss: 0.086 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.006 | Val loss: 0.084 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.005 | Val loss: 0.084 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.005 | Val loss: 0.082 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.004 | Val loss: 0.081 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.004 | Val loss: 0.080 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.004 | Val loss: 0.080 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.003 | Val loss: 0.079 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.003 | Val loss: 0.080 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.003 | Val loss: 0.079 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.003 | Val loss: 0.079 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.003 | Val loss: 0.079 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.002 | Val loss: 0.079 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.002 | Val loss: 0.080 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.002 | Val loss: 0.080 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.002 | Val loss: 0.081 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.002 | Val loss: 0.080 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.002 | Val loss: 0.081 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.002 | Val loss: 0.082 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.002 | Val loss: 0.083 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.002 | Val loss: 0.084 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.001 | Val loss: 0.086 | Gen: ethay airway onditioningcay isway orkingway\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "30e7fc3a-0aa4-4c92-cfc5-49934ac9d944"
      },
      "source": [
        "TEST_SENTENCE = 'but the heat pump is not functioning'\n",
        "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "TEST_SENTENCE2 = 'shake thack ay way sss iii hyperparameters'\n",
        "translated = translate_sentence(TEST_SENTENCE2, rnn_attn_encoder, rnn_attn_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE2, translated))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tbut the heat pump is not functioning \n",
            "translated:\tutbay ethay eathay umppay isway otnay unctioningfay\n",
            "source:\t\tshake thack ay way sss iii hyperparameters \n",
            "translated:\takeshay ackthay ayway ayway sssay iiiiway yzarparametersshay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw_GOIvzo1ix",
        "colab_type": "text"
      },
      "source": [
        "# Part 3: Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Implement Dot-Product Attention\n",
        "Implement the scaled dot product attention module described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_j3oY3hqsJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size(0)\n",
        "        seq_len = keys.size(1)\n",
        "        hidden_size = keys.size(2)\n",
        "        if queries.dim() < 3:\n",
        "            queries = queries.unsqueeze(1)\n",
        "        \n",
        "        q = self.Q(queries) # batch * k * hidden\n",
        "        k = self.K(keys) # batch * seq_len * hidden\n",
        "        v = self.V(values) # batch * seq_len * hidden\n",
        "        unnormalized_attention = (k @ q.transpose(1, 2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = attention_weights.transpose(1, 2) @ v\n",
        "        return context, attention_weights\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Implement Causal Dot-Product Attention\n",
        "Now implement the scaled causal dot product described in the assignment worksheet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovigzQffrKqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        batch_size = keys.size(0)\n",
        "        seq_len = keys.size(1)\n",
        "        hidden_size = keys.size(2)\n",
        "        if queries.dim() < 3:\n",
        "            queries = queries.unsqueeze(1)\n",
        "        q = self.Q(queries) # batch * k * hidden\n",
        "        k = self.K(keys) # batch * seq_len * hidden\n",
        "        v = self.V(values) # batch * seq_len * hidden\n",
        "        unnormalized_attention = (k @ q.transpose(1, 2)) * self.scaling_factor\n",
        "        mask = torch.tril(torch.ones((batch_size, seq_len, seq_len), \n",
        "                                     dtype=torch.uint8)).transpose(1, 2)\n",
        "        unnormalized_attention[mask == 0] = self.neg_inf\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = attention_weights.transpose(1, 2) @ v\n",
        "        return context, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Transformer Encoder\n",
        "Complete the following transformer encoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3B-fWsarlVk",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        \n",
        "        # IMPORTANT CORRECTION: NON-CAUSAL ATTENTION SHOULD HAVE BEEN\n",
        "        # USED IN THE TRANSFORMER ENCODER. \n",
        "        # NEW VERSION: \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        # PREVIONS VERSION: \n",
        "        # self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
        "        #                             hidden_size=hidden_size, \n",
        "        #                          ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        # ------------\n",
        "        # FILL THIS IN - START\n",
        "        # ------------\n",
        "        encoded = self.embedding(inputs) + self.positional_encodings[:seq_len]\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](annotations,\n",
        "                                                                              annotations, \n",
        "                                                                              annotations)  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "        # ------------\n",
        "        # FILL THIS IN - END\n",
        "        # ------------\n",
        "\n",
        "        # Transformer encoder does not have a last hidden layer. \n",
        "        return annotations, None  \n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      if self.opts.cuda:\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1hDi020rT36",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Transformer Decoder\n",
        "Complete the following transformer decoder implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvTZFxtrvc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
        "                                    hidden_size=hidden_size, \n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
        "                                    nn.Linear(hidden_size, hidden_size),\n",
        "                                    nn.ReLU(),\n",
        "                                 ) for i in range(self.num_layers)])\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size \n",
        "\n",
        "        # THIS LINE WAS ADDED AS A CORRECTION. \n",
        "        embed = embed + self.positional_encodings[:seq_len]       \n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "          # ------------\n",
        "          # FILL THIS IN - START\n",
        "          # ------------\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, \n",
        "                                                                           contexts, \n",
        "                                                                           contexts)\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts,\n",
        "                                                                                 annotations,\n",
        "                                                                                 annotations) \n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "          # ------------\n",
        "          # FILL THIS IN - END\n",
        "          # ------------\n",
        "          \n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "          \n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "        \n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "      \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "      Arguments:\n",
        "          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "      Returns:\n",
        "          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
        "      \"\"\"\n",
        "      pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
        "      exponents = (2*dim_indices).float()/(self.hidden_size)\n",
        "      trig_args = pos_indices / (10000**exponents)\n",
        "      sin_terms = torch.sin(trig_args)\n",
        "      cos_terms = torch.cos(trig_args)\n",
        "\n",
        "      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "      pos_encodings[:, 0::2] = sin_terms\n",
        "      pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "      pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "      return pos_encodings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Step 5: Training and analysis\n",
        "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fec90808-85ff-4b8c-8151-fc4c437aa41d"
      },
      "source": [
        "TEST_SENTENCE = 'the air conditioning is working'\n",
        "\n",
        "args = AttrDict()\n",
        "args_dict = {\n",
        "              'cuda':True, \n",
        "              'nepochs':100, \n",
        "              'checkpoint_dir':\"checkpoints\", \n",
        "              'learning_rate':0.005,  ## INCREASE BY AN ORDER OF MAGNITUDE\n",
        "              'lr_decay':0.99,\n",
        "              'batch_size':64, \n",
        "              'hidden_size':20, \n",
        "              'encoder_type': 'transformer',\n",
        "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
        "              'num_transformer_layers': 3,\n",
        "}\n",
        "args.update(args_dict)\n",
        "\n",
        "print_opts(args)\n",
        "transformer_encoder, transformer_decoder = train(args)\n",
        "\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('stage', 'agestay')\n",
            "('breach', 'eachbray')\n",
            "('thirty-six', 'irtythay-ixsay')\n",
            "('lodges', 'odgeslay')\n",
            "('itself', 'itselfway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.161 | Val loss: 2.167 | Gen: ayayayayayayayayayay aaaaaaaaaaaaaaaaaaaa innnninnnnnnnnnnnnnn ayayayayayayayayayay innnnnnnnnnnnnnnnnnn\n",
            "Epoch:   1 | Train loss: 1.670 | Val loss: 1.925 | Gen: aydyyyyyyyyyyyyyyyyy irrrrrrrrrrrrrrrrrrr isisisisisisisisissi isisisisisswwiwwwwwi y\n",
            "Epoch:   2 | Train loss: 1.418 | Val loss: 1.739 | Gen: - iawawwawawawawawawaw ictictitityEOSy iswwwwwwwwwwwwwwwwww ingwwwwwwwwwwwwwwwww\n",
            "Epoch:   3 | Train loss: 1.082 | Val loss: 1.289 | Gen: ayayayayayaayyaEOSaEOSay aaaaaawwwaaawaaaawwa atayctctctctay isssssssssssssssssss asssssssssssssssssss\n",
            "Epoch:   4 | Train loss: 0.516 | Val loss: 0.843 | Gen: y aaaaay aaaaay aaaay aaaaay\n",
            "Epoch:   5 | Train loss: 0.244 | Val loss: 0.803 | Gen: emmmmmmmmmmmmmmmmmmm aaaaaaaaaaaaaaaaaay ommppiiiipiiiEOSmmppim  iepwwawwpppwiwwwwwww\n",
            "Epoch:   6 | Train loss: 0.194 | Val loss: 0.200 | Gen: aaaaay aaaaay ayoooooy aaaaay aaaaay\n",
            "Epoch:   7 | Train loss: 0.058 | Val loss: 0.175 | Gen: aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay aoooooooooooommmmmo aaaaaaaaaaaaaaaaaay aooooooooooooy\n",
            "Epoch:   8 | Train loss: 0.049 | Val loss: 0.086 | Gen: mmmmmmmmmmmmmmmmmmmm aayooooooooooy omoooooooooooooooomo ommmmmmmmmmmmmmmmmmo omooooooooooooooooy\n",
            "Epoch:   9 | Train loss: 0.032 | Val loss: 0.130 | Gen: aaaaaaaaay aaaaaaaaaaaaaaaaay aaoaaaaay aaaaaaaaaaaaaaaaay aaaaaayyy\n",
            "Epoch:  10 | Train loss: 0.030 | Val loss: 0.229 | Gen: aaaoadrrrrrrrrrrmama -oooaaalllllallloma- a-oaa a-oaay aaoaondndnddnnafay\n",
            "Epoch:  11 | Train loss: 0.047 | Val loss: 0.068 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaalllalalllllllaal aaaaaaaaaaaaaaaaaana\n",
            "Epoch:  12 | Train loss: 0.049 | Val loss: 0.563 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  13 | Train loss: 0.111 | Val loss: 0.041 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa oooooooooooooooooooo aaaaaaaaaaaaaaaaaay oooooooooooooooooooo\n",
            "Epoch:  14 | Train loss: 0.011 | Val loss: 0.015 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa oooooooooooooooooooo aaaaaaaaaaaaaaaaaay oooooooooooooooooooo\n",
            "Epoch:  15 | Train loss: 0.011 | Val loss: 0.020 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa oooooooooooooooooooo aaaaaaaaaaaaaaaaaay oooooooooooooooooonm\n",
            "Epoch:  16 | Train loss: 0.017 | Val loss: 0.025 | Gen: mmmmmmmmmmmmmmmmmmam oooooooooooooooooooo oooooooooooooooooooo oooooooooooooooooooo oooooooooooooooooooo\n",
            "Epoch:  17 | Train loss: 0.092 | Val loss: 1.050 | Gen: aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa yyffffffffffffayffay aaaaaaaaaaaaaaaaayym\n",
            "Epoch:  18 | Train loss: 0.114 | Val loss: 0.156 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  19 | Train loss: 0.020 | Val loss: 0.026 | Gen: ooooooooooooooooooon aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  20 | Train loss: 0.049 | Val loss: 0.041 | Gen: -------------------- -------------------- aaaaaaaaaaaaaaaaaaaa -------------------- --------------------\n",
            "Epoch:  21 | Train loss: 0.012 | Val loss: 0.042 | Gen: aaaaaaaaaaaaaaaaaaaa -   aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  22 | Train loss: 0.077 | Val loss: 0.235 | Gen: aaaaaaaaaaaaaaaaaaaa --------yy ----------------aaal aaaaaaaaaaaaaaaaaaaa --------------------\n",
            "Epoch:  23 | Train loss: 0.074 | Val loss: 0.105 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  24 | Train loss: 0.026 | Val loss: 0.085 | Gen: --- aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay\n",
            "Epoch:  25 | Train loss: 0.017 | Val loss: 0.021 | Gen: aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay\n",
            "Epoch:  26 | Train loss: 0.013 | Val loss: 0.024 | Gen: aaaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaaay aaaaaaaaaaaaaaaaaay\n",
            "Epoch:  27 | Train loss: 0.182 | Val loss: 0.230 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa ennnnnnnnnnnnnnnnnsn aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  28 | Train loss: 0.119 | Val loss: 0.507 | Gen: -------------------- -------------------- oooooooooooooooooooo -------------------- --------------------\n",
            "Epoch:  29 | Train loss: 0.126 | Val loss: 0.585 | Gen: aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  30 | Train loss: 0.086 | Val loss: 0.085 | Gen: aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  31 | Train loss: 0.052 | Val loss: 0.051 | Gen: alaaaaaaaaaaalkkkkk aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  32 | Train loss: 0.020 | Val loss: 0.032 | Gen: aaaaay aaaaaaaaaaaaaaaaaaaa aaaaay  aaaaay\n",
            "Epoch:  33 | Train loss: 0.009 | Val loss: 0.023 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  34 | Train loss: 0.002 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  35 | Train loss: 0.001 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  36 | Train loss: 0.001 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  37 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  38 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  39 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  40 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  41 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  42 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  43 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  44 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  45 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  46 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  47 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  48 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  49 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  50 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  51 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  52 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  53 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  54 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  55 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  56 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  57 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  58 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  59 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  60 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  61 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  62 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  63 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  64 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  65 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  66 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  67 | Train loss: 0.000 | Val loss: 0.016 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  68 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  69 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  70 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  71 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  72 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  73 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  74 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  75 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  76 | Train loss: 0.000 | Val loss: 0.017 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  77 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  78 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  79 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  80 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  81 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  82 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  83 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  84 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  85 | Train loss: 0.000 | Val loss: 0.018 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  86 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  87 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  88 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  89 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  90 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  91 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  92 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  93 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  94 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  95 | Train loss: 0.000 | Val loss: 0.019 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  96 | Train loss: 0.000 | Val loss: 0.020 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  97 | Train loss: 0.000 | Val loss: 0.020 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  98 | Train loss: 0.000 | Val loss: 0.020 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "Epoch:  99 | Train loss: 0.000 | Val loss: 0.020 | Gen:  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\t aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa  aaaaaaaaaaaaaaaaaaaa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R18s80gzC6A8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ffd76871-ac03-4216-9e56-884f3adc035d"
      },
      "source": [
        "TEST_SENTENCE = 'but the heat pump is not functioning'\n",
        "translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
        "TEST_SENTENCE2 = 'shake thack ay way sss iii hyperparameters'\n",
        "translated = translate_sentence(TEST_SENTENCE2, transformer_encoder, transformer_decoder, None, args)\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE2, translated))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\tbut the heat pump is not functioning \n",
            "translated:\t  aaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaa   aaaaaaaaaaaaaaaaaaaa\n",
            "source:\t\tshake thack ay way sss iii hyperparameters \n",
            "translated:\taaaaaaaaaaaaaaaaaaaa      aaaaaaaaaaaaaaaaaaaa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBnBXRG8mvcn",
        "colab_type": "text"
      },
      "source": [
        "# Optional: Attention Visualizations\n",
        "\n",
        "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
        "\n",
        "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
        "\n",
        "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEC0vN9mvpV",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Visualize Attention Masks\n",
        "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkfz-u-MtudL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssa7g35zt2yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_WORD_ATTN = 'street'\n",
        "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owstslMF-wdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}